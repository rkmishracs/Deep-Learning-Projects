{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gesture Recognition\n",
    "## Ram Krishn Mishra\n",
    "## Sairaz Kota"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the following libraries to get started.\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy.misc import imread, imresize\n",
    "import cv2\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to setup the random seed so that the results don't vary drastically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(30)\n",
    "\n",
    "import random as rn\n",
    "rn.seed(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will read the folder names for training and validation. We also set the `batch_size` here. Note that we need to set the batch size in such a way that we are able to use the GPU in full capacity. we keep increasing the batch size until the machine throws an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc = np.random.permutation(open('./Project_data/train.csv').readlines())\n",
    "val_doc = np.random.permutation(open('./Project_data/val.csv').readlines())\n",
    "batch_size = 10 #experiment with the batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is one of the most important part of the code. The overall structure of the generator has been given. In the generator, we are going to preprocess the images as we have images of 2 different dimensions as well as create a batch of video frames. we have to experiment with `img_idx`, `y`,`z` and normalization such that we get high accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the params\n",
    "\n",
    "nb_rows = 120   # X dimension of the image\n",
    "nb_cols = 120   # Y dimesnion of the image\n",
    "\n",
    "#total_frames = 30\n",
    "\n",
    "nb_frames = 30  # length of the video frames\n",
    "nb_channel = 3 # number of channels in images 3 for RGB color & 1 for Grey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Util function to initialize all the batch image labels and respective data\n",
    "\n",
    "def init_batch_data(batch_size):\n",
    "    batch_data = np.zeros((batch_size, nb_frames, nb_rows, nb_cols, nb_channel)) \n",
    "    batch_labels = np.zeros((batch_size,5)) # batch_labels is the one best representation of the output\n",
    "    return batch_data, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Util function to normalise the data\n",
    "\n",
    "def normalize_data(data):\n",
    "    return data/127.5-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Util function to create/ generate a random affine transformation on the image\n",
    "\n",
    "def get_random_affine():\n",
    "    dx, dy = np.random.randint(-1.7, 1.8, 2)\n",
    "    M = np.float32([[1, 0, dx], [0, 1, dy]])\n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_batch_images(source_path, folder_list, batch_num, batch_size, t, validation):\n",
    "    \n",
    "    batch_data, batch_labels = init_batch_data(batch_size)\n",
    "    \n",
    "    # We will also build an augumented batch data with affine transformation\n",
    "    batch_data_aug, batch_labels_aug = init_batch_data(batch_size)\n",
    "    \n",
    "    # We will also build an augmented batch data with horizontal flip\n",
    "    batch_data_flip, batch_labels_flip = init_batch_data(batch_size)\n",
    "    \n",
    "    # Lets create a list of image numbers you want to use for a particular video using full frames\n",
    "    img_idx = [x for x in range(0, nb_frames)]\n",
    "    \n",
    "\n",
    "    for folder in range(batch_size):\n",
    "        # Iterate all the images in the folder with the batch size\n",
    "        \n",
    "        imgs = sorted(os.listdir(source_path+'/'+ t[folder + (batch_num * batch_size)].split(';')[0]))\n",
    "        \n",
    "        # Generate a random affine transformation to be used in image transformation for -\n",
    "        #buidling agumented data set\n",
    "        \n",
    "        M = get_random_affine()\n",
    "        \n",
    "        \n",
    "        \n",
    "        for idx, item in enumerate(img_idx): \n",
    "            #  Iterate over the frames/images of a folder to read them in\n",
    "            image = cv2.imread(source_path+'/'+ t[folder + (batch_num*batch_size)].strip().split(';')[0]+'/'+imgs[item], cv2.IMREAD_COLOR)\n",
    "            \n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Lets crop non symmetric frames\n",
    "            if image.shape[0] != image.shape[1]:\n",
    "                image = image[0:120,20:140]\n",
    "            \n",
    "            # Crop the images and resize them as the images are of 2 different shape & the conv3D will throw error \n",
    "            # If the inputs are in a batch with different shapes   \n",
    "            resized = cv2.resize(image, (nb_rows,nb_cols), interpolation = cv2.INTER_AREA)\n",
    "            \n",
    "            # Normalize data\n",
    "            batch_data[folder, idx] = (resized)\n",
    "            \n",
    "            # Data with affine transformation\n",
    "            batch_data_aug[folder, idx] = (cv2.warpAffine(resized, M, (resized.shape[0], resized.shape[1])))\n",
    "            \n",
    "            # Data with horizontal flip\n",
    "            batch_data_flip[folder, idx]= np.flip(resized, 1)\n",
    "\n",
    "        batch_labels[folder, int(t[folder + (batch_num*batch_size)].strip().split(';')[2])] = 1\n",
    "        batch_labels_aug[folder, int(t[folder + (batch_num*batch_size)].strip().split(';')[2])] = 1\n",
    "        \n",
    "        # Labeling data with horizobtal flip, right swipe becomes left swipe & viceversa\n",
    "        if int(t[folder + (batch_num*batch_size)].strip().split(';')[2])==0:\n",
    "                    batch_labels_flip[folder, 1] = 1\n",
    "        elif int(t[folder + (batch_num*batch_size)].strip().split(';')[2])==1:\n",
    "                    batch_labels_flip[folder, 0] = 1\n",
    "                    \n",
    "        else:\n",
    "                    batch_labels_flip[folder, int(t[folder + (batch_num*batch_size)].strip().split(';')[2])] = 1\n",
    "                  \n",
    "    \n",
    "    batch_data_final = np.append(batch_data, batch_data_aug, axis = 0)\n",
    "    batch_data_final = np.append(batch_data_final, batch_data_flip, axis = 0)\n",
    "\n",
    "    batch_labels_final = np.append(batch_labels, batch_labels_aug, axis = 0) \n",
    "    batch_labels_final = np.append(batch_labels_final, batch_labels_flip, axis = 0)\n",
    "    \n",
    "    if validation:\n",
    "        batch_data_final=batch_data\n",
    "        batch_labels_final= batch_labels\n",
    "        \n",
    "    return batch_data_final,batch_labels_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(source_path, folder_list, batch_size, validation=False):\n",
    "    \n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    \n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = len(folder_list)//batch_size # calculate the number of batches\n",
    "        \n",
    "        for batch in range(num_batches):        # we iterate over the number of batches\n",
    "            \n",
    "            # we yield the batch_data and the batch_labels, remember what does yield do\n",
    "            yield load_batch_images(source_path, folder_list, batch, batch_size, t,validation)\n",
    "            \n",
    "\n",
    "        \n",
    "        # For the remaining data points which are left after full batches\n",
    "        if (len(folder_list) != batch_size*num_batches):\n",
    "            \n",
    "            batch_size = len(folder_list) - (batch_size*num_batches)\n",
    "            yield load_batch_images(source_path, folder_list, batch, batch_size, t,validation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the generator as (number of images, height, width, number of channels). we need take this into consideration while creating the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "\n",
    "train_path = './Project_data/train'\n",
    "\n",
    "val_path = './Project_data/val'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of training sequences = 663\n"
     ]
    }
   ],
   "source": [
    "num_train_sequences = len(train_doc)\n",
    "print('No of training sequences =', num_train_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of validation sequences = 100\n"
     ]
    }
   ],
   "source": [
    "num_val_sequences = len(val_doc)\n",
    "print('No of validation sequences =', num_val_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 10 # the number of epochs\n",
    "num_epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "Here we make the model using different functionalities that Keras provides. We have to use `Conv3D` and `MaxPooling3D` and not `Conv2D` and `Maxpooling2D` for a 3D convolution model. we would want to use `TimeDistributed` while building a Conv2D + RNN model. Also we need that the last layer is the softmax. we need to design the network in such a way that the model is able to give good accuracy on the least number of parameters so that it can fit in the memory of the webcam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation, Dropout\n",
    "from keras.layers.convolutional import Conv3D, MaxPooling3D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_filters = [8,16,32,64]\n",
    "nb_dense = [256, 128, 5]\n",
    "\n",
    "# Input\n",
    "input_shape=(30,nb_rows,nb_cols,3)\n",
    "\n",
    "# Define model\n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "model.add(Conv3D(nb_filters[0], \n",
    "                 kernel_size=(3,3,3), \n",
    "                 input_shape=input_shape,\n",
    "                 padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "\n",
    "model.add(Conv3D(nb_filters[1], \n",
    "                 kernel_size=(3,3,3), \n",
    "                 padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "\n",
    "\n",
    "model.add(Conv3D(nb_filters[2], \n",
    "                 kernel_size=(1,3,3), \n",
    "                 padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "\n",
    "\n",
    "model.add(Conv3D(nb_filters[3], \n",
    "                 kernel_size=(1,3,3), \n",
    "                 padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flatten Layers\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(nb_dense[0], activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(nb_dense[1], activation='relu'))\n",
    "model.add(Dropout(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#softmax layer\n",
    "\n",
    "model.add(Dense(nb_dense[2], activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have written the model, the next step is to `compile` the model. When we print the `summary` of the model, we'll see the total number of parameters that we have to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_1 (Conv3D)            (None, 30, 120, 120, 8)   656       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 30, 120, 120, 8)   32        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 30, 120, 120, 8)   0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3 (None, 15, 60, 60, 8)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_2 (Conv3D)            (None, 15, 60, 60, 16)    3472      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 15, 60, 60, 16)    64        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 15, 60, 60, 16)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_2 (MaxPooling3 (None, 7, 30, 30, 16)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_3 (Conv3D)            (None, 7, 30, 30, 32)     4640      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 7, 30, 30, 32)     128       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 7, 30, 30, 32)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_3 (MaxPooling3 (None, 3, 15, 15, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_4 (Conv3D)            (None, 3, 15, 15, 64)     18496     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 3, 15, 15, 64)     0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 3, 15, 15, 64)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_4 (MaxPooling3 (None, 1, 7, 7, 64)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               803072    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 864,101\n",
      "Trainable params: 863,989\n",
      "Non-trainable params: 112\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "optimiser = Adam() #optimizer to avoid the perfomance issue\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create the `train_generator` and the `val_generator` which will be used in `.fit_generator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, cooldown=1, verbose=1)# write the REducelronplateau code here\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "    \n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "    \n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now fit the model. This will start training the model and with the help of the checkpoints, we'll be able to save the model at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Source path =  ./Project_data/val ; batch size = 10\n",
      "Source path =  ./Project_data/train ; batch size = 10\n",
      "67/67 [==============================] - 282s 4s/step - loss: 1.6768 - categorical_accuracy: 0.2916 - val_loss: 2.6635 - val_categorical_accuracy: 0.3333\n",
      "\n",
      "Epoch 00001: saving model to model_init_2020-05-0415_24_46.301761/model-00001-1.68391-0.29160-2.66349-0.33333.h5\n",
      "Epoch 2/10\n",
      "67/67 [==============================] - 117s 2s/step - loss: 1.3827 - categorical_accuracy: 0.3947 - val_loss: 1.4352 - val_categorical_accuracy: 0.4967\n",
      "\n",
      "Epoch 00002: saving model to model_init_2020-05-0415_24_46.301761/model-00002-1.38271-0.39469-1.43517-0.49667.h5\n",
      "Epoch 3/10\n",
      "67/67 [==============================] - 116s 2s/step - loss: 1.3589 - categorical_accuracy: 0.4693 - val_loss: 1.3318 - val_categorical_accuracy: 0.4833\n",
      "\n",
      "Epoch 00003: saving model to model_init_2020-05-0415_24_46.301761/model-00003-1.35887-0.46932-1.33183-0.48333.h5\n",
      "Epoch 4/10\n",
      "67/67 [==============================] - 116s 2s/step - loss: 1.2601 - categorical_accuracy: 0.4561 - val_loss: 1.5177 - val_categorical_accuracy: 0.4600\n",
      "\n",
      "Epoch 00004: saving model to model_init_2020-05-0415_24_46.301761/model-00004-1.26008-0.45605-1.51766-0.46000.h5\n",
      "Epoch 5/10\n",
      "67/67 [==============================] - 119s 2s/step - loss: 1.1997 - categorical_accuracy: 0.5290 - val_loss: 1.2005 - val_categorical_accuracy: 0.5733\n",
      "\n",
      "Epoch 00005: saving model to model_init_2020-05-0415_24_46.301761/model-00005-1.19965-0.52902-1.20052-0.57333.h5\n",
      "Epoch 6/10\n",
      "67/67 [==============================] - 116s 2s/step - loss: 1.0832 - categorical_accuracy: 0.5274 - val_loss: 1.0920 - val_categorical_accuracy: 0.6433\n",
      "\n",
      "Epoch 00006: saving model to model_init_2020-05-0415_24_46.301761/model-00006-1.08324-0.52736-1.09199-0.64333.h5\n",
      "Epoch 7/10\n",
      "67/67 [==============================] - 119s 2s/step - loss: 1.1545 - categorical_accuracy: 0.5290 - val_loss: 1.1044 - val_categorical_accuracy: 0.5067\n",
      "\n",
      "Epoch 00007: saving model to model_init_2020-05-0415_24_46.301761/model-00007-1.15454-0.52902-1.10439-0.50667.h5\n",
      "Epoch 8/10\n",
      "67/67 [==============================] - 117s 2s/step - loss: 1.1182 - categorical_accuracy: 0.5506 - val_loss: 1.3103 - val_categorical_accuracy: 0.5700\n",
      "\n",
      "Epoch 00008: saving model to model_init_2020-05-0415_24_46.301761/model-00008-1.11820-0.55058-1.31032-0.57000.h5\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 9/10\n",
      "67/67 [==============================] - 117s 2s/step - loss: 0.9040 - categorical_accuracy: 0.6418 - val_loss: 0.7230 - val_categorical_accuracy: 0.6867\n",
      "\n",
      "Epoch 00009: saving model to model_init_2020-05-0415_24_46.301761/model-00009-0.90398-0.64179-0.72299-0.68667.h5\n",
      "Epoch 10/10\n",
      "67/67 [==============================] - 119s 2s/step - loss: 0.8324 - categorical_accuracy: 0.6501 - val_loss: 0.8344 - val_categorical_accuracy: 0.6600\n",
      "\n",
      "Epoch 00010: saving model to model_init_2020-05-0415_24_46.301761/model-00010-0.83240-0.65008-0.83440-0.66000.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fb4701d4c10>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "34/34 [==============================] - 61s 2s/step - loss: 0.8669 - categorical_accuracy: 0.6438 - val_loss: 0.7043 - val_categorical_accuracy: 0.7667\n",
      "\n",
      "Epoch 00001: saving model to model_init_2020-05-0415_24_46.301761/model-00001-0.86686-0.64379-0.70434-0.76667.h5\n",
      "Epoch 2/20\n",
      "34/34 [==============================] - 58s 2s/step - loss: 0.7513 - categorical_accuracy: 0.7190 - val_loss: 0.9429 - val_categorical_accuracy: 0.5733\n",
      "\n",
      "Epoch 00002: saving model to model_init_2020-05-0415_24_46.301761/model-00002-0.75130-0.71895-0.94286-0.57333.h5\n",
      "Epoch 3/20\n",
      "34/34 [==============================] - 58s 2s/step - loss: 0.6723 - categorical_accuracy: 0.7059 - val_loss: 0.8812 - val_categorical_accuracy: 0.7133\n",
      "\n",
      "Epoch 00003: saving model to model_init_2020-05-0415_24_46.301761/model-00003-0.67228-0.70588-0.88124-0.71333.h5\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 4/20\n",
      "34/34 [==============================] - 58s 2s/step - loss: 0.7773 - categorical_accuracy: 0.6699 - val_loss: 1.1940 - val_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00004: saving model to model_init_2020-05-0415_24_46.301761/model-00004-0.77725-0.66993-1.19401-0.70000.h5\n",
      "Epoch 5/20\n",
      "34/34 [==============================] - 59s 2s/step - loss: 0.6520 - categorical_accuracy: 0.7353 - val_loss: 0.5950 - val_categorical_accuracy: 0.7733\n",
      "\n",
      "Epoch 00005: saving model to model_init_2020-05-0415_24_46.301761/model-00005-0.65199-0.73529-0.59501-0.77333.h5\n",
      "Epoch 6/20\n",
      "34/34 [==============================] - 59s 2s/step - loss: 0.6651 - categorical_accuracy: 0.7255 - val_loss: 0.9097 - val_categorical_accuracy: 0.7067\n",
      "\n",
      "Epoch 00006: saving model to model_init_2020-05-0415_24_46.301761/model-00006-0.66509-0.72549-0.90966-0.70667.h5\n",
      "Epoch 7/20\n",
      "34/34 [==============================] - 59s 2s/step - loss: 0.6520 - categorical_accuracy: 0.7386 - val_loss: 0.4923 - val_categorical_accuracy: 0.7067\n",
      "\n",
      "Epoch 00007: saving model to model_init_2020-05-0415_24_46.301761/model-00007-0.65200-0.73856-0.49229-0.70667.h5\n",
      "Epoch 8/20\n",
      "34/34 [==============================] - 59s 2s/step - loss: 0.7062 - categorical_accuracy: 0.7124 - val_loss: 0.5930 - val_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00008: saving model to model_init_2020-05-0415_24_46.301761/model-00008-0.70620-0.71242-0.59299-0.70000.h5\n",
      "Epoch 9/20\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.5677 - categorical_accuracy: 0.7778 - val_loss: 0.6365 - val_categorical_accuracy: 0.7400\n",
      "\n",
      "Epoch 00009: saving model to model_init_2020-05-0415_24_46.301761/model-00009-0.56769-0.77778-0.63651-0.74000.h5\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 10/20\n",
      "34/34 [==============================] - 59s 2s/step - loss: 0.6154 - categorical_accuracy: 0.7778 - val_loss: 0.6120 - val_categorical_accuracy: 0.7667\n",
      "\n",
      "Epoch 00010: saving model to model_init_2020-05-0415_24_46.301761/model-00010-0.61544-0.77778-0.61195-0.76667.h5\n",
      "Epoch 11/20\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.5321 - categorical_accuracy: 0.7745 - val_loss: 0.4020 - val_categorical_accuracy: 0.8067\n",
      "\n",
      "Epoch 00011: saving model to model_init_2020-05-0415_24_46.301761/model-00011-0.53213-0.77451-0.40202-0.80667.h5\n",
      "Epoch 12/20\n",
      "34/34 [==============================] - 61s 2s/step - loss: 0.5368 - categorical_accuracy: 0.7876 - val_loss: 0.5808 - val_categorical_accuracy: 0.7600\n",
      "\n",
      "Epoch 00012: saving model to model_init_2020-05-0415_24_46.301761/model-00012-0.53685-0.78758-0.58081-0.76000.h5\n",
      "Epoch 13/20\n",
      "34/34 [==============================] - 59s 2s/step - loss: 0.5063 - categorical_accuracy: 0.8137 - val_loss: 0.5657 - val_categorical_accuracy: 0.7333\n",
      "\n",
      "Epoch 00013: saving model to model_init_2020-05-0415_24_46.301761/model-00013-0.50627-0.81373-0.56572-0.73333.h5\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 14/20\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.4774 - categorical_accuracy: 0.8105 - val_loss: 0.9242 - val_categorical_accuracy: 0.7067\n",
      "\n",
      "Epoch 00014: saving model to model_init_2020-05-0415_24_46.301761/model-00014-0.47738-0.81046-0.92419-0.70667.h5\n",
      "Epoch 15/20\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.5600 - categorical_accuracy: 0.7680 - val_loss: 0.5457 - val_categorical_accuracy: 0.7933\n",
      "\n",
      "Epoch 00015: saving model to model_init_2020-05-0415_24_46.301761/model-00015-0.56003-0.76797-0.54572-0.79333.h5\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 16/20\n",
      "34/34 [==============================] - 59s 2s/step - loss: 0.4256 - categorical_accuracy: 0.8235 - val_loss: 0.9469 - val_categorical_accuracy: 0.7600\n",
      "\n",
      "Epoch 00016: saving model to model_init_2020-05-0415_24_46.301761/model-00016-0.42563-0.82353-0.94690-0.76000.h5\n",
      "Epoch 17/20\n",
      "34/34 [==============================] - 58s 2s/step - loss: 0.5302 - categorical_accuracy: 0.7974 - val_loss: 0.3597 - val_categorical_accuracy: 0.7533\n",
      "\n",
      "Epoch 00017: saving model to model_init_2020-05-0415_24_46.301761/model-00017-0.53025-0.79739-0.35967-0.75333.h5\n",
      "Epoch 18/20\n",
      "34/34 [==============================] - 59s 2s/step - loss: 0.5334 - categorical_accuracy: 0.7745 - val_loss: 0.3242 - val_categorical_accuracy: 0.8267\n",
      "\n",
      "Epoch 00018: saving model to model_init_2020-05-0415_24_46.301761/model-00018-0.53335-0.77451-0.32423-0.82667.h5\n",
      "Epoch 19/20\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.4981 - categorical_accuracy: 0.7941 - val_loss: 0.3888 - val_categorical_accuracy: 0.7533\n",
      "\n",
      "Epoch 00019: saving model to model_init_2020-05-0415_24_46.301761/model-00019-0.49809-0.79412-0.38879-0.75333.h5\n",
      "Epoch 20/20\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.4623 - categorical_accuracy: 0.8072 - val_loss: 0.1149 - val_categorical_accuracy: 0.8000\n",
      "\n",
      "Epoch 00020: saving model to model_init_2020-05-0415_24_46.301761/model-00020-0.46231-0.80719-0.11492-0.80000.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fb4700421d0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 20\n",
    "num_epochs = 20\n",
    "\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1\n",
    "    \n",
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "23/23 [==============================] - 42s 2s/step - loss: 0.4974 - categorical_accuracy: 0.7971 - val_loss: 0.4490 - val_categorical_accuracy: 0.7417\n",
      "\n",
      "Epoch 00001: saving model to model_init_2020-05-0415_24_46.301761/model-00001-0.49745-0.79710-0.44897-0.74167.h5\n",
      "Epoch 2/30\n",
      "23/23 [==============================] - 41s 2s/step - loss: 0.4412 - categorical_accuracy: 0.8261 - val_loss: 0.8202 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00002: saving model to model_init_2020-05-0415_24_46.301761/model-00002-0.44120-0.82609-0.82019-0.75000.h5\n",
      "Epoch 3/30\n",
      "23/23 [==============================] - 41s 2s/step - loss: 0.5114 - categorical_accuracy: 0.7729 - val_loss: 0.6415 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00003: saving model to model_init_2020-05-0415_24_46.301761/model-00003-0.51137-0.77295-0.64149-0.75000.h5\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 3.814697446813398e-09.\n",
      "Epoch 4/30\n",
      "23/23 [==============================] - 41s 2s/step - loss: 0.4935 - categorical_accuracy: 0.8116 - val_loss: 0.4157 - val_categorical_accuracy: 0.8083\n",
      "\n",
      "Epoch 00004: saving model to model_init_2020-05-0415_24_46.301761/model-00004-0.49346-0.81159-0.41574-0.80833.h5\n",
      "Epoch 5/30\n",
      "23/23 [==============================] - 40s 2s/step - loss: 0.4416 - categorical_accuracy: 0.7971 - val_loss: 0.3677 - val_categorical_accuracy: 0.7667\n",
      "\n",
      "Epoch 00005: saving model to model_init_2020-05-0415_24_46.301761/model-00005-0.44158-0.79710-0.36767-0.76667.h5\n",
      "Epoch 6/30\n",
      "23/23 [==============================] - 40s 2s/step - loss: 0.4391 - categorical_accuracy: 0.8261 - val_loss: 0.8394 - val_categorical_accuracy: 0.7333\n",
      "\n",
      "Epoch 00006: saving model to model_init_2020-05-0415_24_46.301761/model-00006-0.43907-0.82609-0.83944-0.73333.h5\n",
      "Epoch 7/30\n",
      "23/23 [==============================] - 41s 2s/step - loss: 0.4609 - categorical_accuracy: 0.8068 - val_loss: 0.5006 - val_categorical_accuracy: 0.8167\n",
      "\n",
      "Epoch 00007: saving model to model_init_2020-05-0415_24_46.301761/model-00007-0.46090-0.80676-0.50057-0.81667.h5\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 1.907348723406699e-09.\n",
      "Epoch 8/30\n",
      "23/23 [==============================] - 40s 2s/step - loss: 0.5714 - categorical_accuracy: 0.7585 - val_loss: 0.4257 - val_categorical_accuracy: 0.7750\n",
      "\n",
      "Epoch 00008: saving model to model_init_2020-05-0415_24_46.301761/model-00008-0.57138-0.75845-0.42571-0.77500.h5\n",
      "Epoch 9/30\n",
      "23/23 [==============================] - 41s 2s/step - loss: 0.4850 - categorical_accuracy: 0.8213 - val_loss: 0.3316 - val_categorical_accuracy: 0.8333\n",
      "\n",
      "Epoch 00009: saving model to model_init_2020-05-0415_24_46.301761/model-00009-0.48499-0.82126-0.33162-0.83333.h5\n",
      "Epoch 10/30\n",
      "23/23 [==============================] - 41s 2s/step - loss: 0.5396 - categorical_accuracy: 0.8068 - val_loss: 0.6880 - val_categorical_accuracy: 0.6917\n",
      "\n",
      "Epoch 00010: saving model to model_init_2020-05-0415_24_46.301761/model-00010-0.53959-0.80676-0.68800-0.69167.h5\n",
      "Epoch 11/30\n",
      "23/23 [==============================] - 40s 2s/step - loss: 0.4993 - categorical_accuracy: 0.8357 - val_loss: 0.4785 - val_categorical_accuracy: 0.7750\n",
      "\n",
      "Epoch 00011: saving model to model_init_2020-05-0415_24_46.301761/model-00011-0.49931-0.83575-0.47850-0.77500.h5\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.536743617033494e-10.\n",
      "Epoch 12/30\n",
      "23/23 [==============================] - 42s 2s/step - loss: 0.3545 - categorical_accuracy: 0.8696 - val_loss: 0.4317 - val_categorical_accuracy: 0.7417\n",
      "\n",
      "Epoch 00012: saving model to model_init_2020-05-0415_24_46.301761/model-00012-0.35451-0.86957-0.43175-0.74167.h5\n",
      "Epoch 13/30\n",
      "23/23 [==============================] - 42s 2s/step - loss: 0.4406 - categorical_accuracy: 0.8213 - val_loss: 0.6196 - val_categorical_accuracy: 0.7750\n",
      "\n",
      "Epoch 00013: saving model to model_init_2020-05-0415_24_46.301761/model-00013-0.44057-0.82126-0.61965-0.77500.h5\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 4.768371808516747e-10.\n",
      "Epoch 14/30\n",
      "23/23 [==============================] - 41s 2s/step - loss: 0.5306 - categorical_accuracy: 0.8261 - val_loss: 0.5691 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00014: saving model to model_init_2020-05-0415_24_46.301761/model-00014-0.53065-0.82609-0.56912-0.75000.h5\n",
      "Epoch 15/30\n",
      "23/23 [==============================] - 40s 2s/step - loss: 0.3999 - categorical_accuracy: 0.8454 - val_loss: 0.6317 - val_categorical_accuracy: 0.7917\n",
      "\n",
      "Epoch 00015: saving model to model_init_2020-05-0415_24_46.301761/model-00015-0.39993-0.84541-0.63169-0.79167.h5\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 2.3841859042583735e-10.\n",
      "Epoch 16/30\n",
      "23/23 [==============================] - 41s 2s/step - loss: 0.5660 - categorical_accuracy: 0.7488 - val_loss: 0.7425 - val_categorical_accuracy: 0.7417\n",
      "\n",
      "Epoch 00016: saving model to model_init_2020-05-0415_24_46.301761/model-00016-0.56603-0.74879-0.74247-0.74167.h5\n",
      "Epoch 17/30\n",
      "23/23 [==============================] - 41s 2s/step - loss: 0.5337 - categorical_accuracy: 0.7874 - val_loss: 0.3836 - val_categorical_accuracy: 0.7750\n",
      "\n",
      "Epoch 00017: saving model to model_init_2020-05-0415_24_46.301761/model-00017-0.53374-0.78744-0.38364-0.77500.h5\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.1920929521291868e-10.\n",
      "Epoch 18/30\n",
      "23/23 [==============================] - 41s 2s/step - loss: 0.4006 - categorical_accuracy: 0.8599 - val_loss: 0.7995 - val_categorical_accuracy: 0.8250\n",
      "\n",
      "Epoch 00018: saving model to model_init_2020-05-0415_24_46.301761/model-00018-0.40065-0.85990-0.79954-0.82500.h5\n",
      "Epoch 19/30\n",
      "23/23 [==============================] - 40s 2s/step - loss: 0.4405 - categorical_accuracy: 0.8406 - val_loss: 0.5607 - val_categorical_accuracy: 0.8083\n",
      "\n",
      "Epoch 00019: saving model to model_init_2020-05-0415_24_46.301761/model-00019-0.44052-0.84058-0.56073-0.80833.h5\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 5.960464760645934e-11.\n",
      "Epoch 20/30\n",
      "23/23 [==============================] - 40s 2s/step - loss: 0.3869 - categorical_accuracy: 0.8309 - val_loss: 1.1394 - val_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00020: saving model to model_init_2020-05-0415_24_46.301761/model-00020-0.38693-0.83092-1.13937-0.70000.h5\n",
      "Epoch 21/30\n",
      "23/23 [==============================] - 42s 2s/step - loss: 0.4875 - categorical_accuracy: 0.8357 - val_loss: 0.4503 - val_categorical_accuracy: 0.7250\n",
      "\n",
      "Epoch 00021: saving model to model_init_2020-05-0415_24_46.301761/model-00021-0.48746-0.83575-0.45032-0.72500.h5\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 2.980232380322967e-11.\n",
      "Epoch 22/30\n",
      "23/23 [==============================] - 40s 2s/step - loss: 0.4985 - categorical_accuracy: 0.7826 - val_loss: 0.4732 - val_categorical_accuracy: 0.8250\n",
      "\n",
      "Epoch 00022: saving model to model_init_2020-05-0415_24_46.301761/model-00022-0.49845-0.78261-0.47323-0.82500.h5\n",
      "Epoch 23/30\n",
      "23/23 [==============================] - 41s 2s/step - loss: 0.3822 - categorical_accuracy: 0.8551 - val_loss: 0.7256 - val_categorical_accuracy: 0.7417\n",
      "\n",
      "Epoch 00023: saving model to model_init_2020-05-0415_24_46.301761/model-00023-0.38215-0.85507-0.72564-0.74167.h5\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.4901161901614834e-11.\n",
      "Epoch 24/30\n",
      "23/23 [==============================] - 41s 2s/step - loss: 0.4998 - categorical_accuracy: 0.7729 - val_loss: 0.3583 - val_categorical_accuracy: 0.8583\n",
      "\n",
      "Epoch 00024: saving model to model_init_2020-05-0415_24_46.301761/model-00024-0.49982-0.77295-0.35825-0.85833.h5\n",
      "Epoch 25/30\n",
      "23/23 [==============================] - 41s 2s/step - loss: 0.5688 - categorical_accuracy: 0.7391 - val_loss: 0.5492 - val_categorical_accuracy: 0.6583\n",
      "\n",
      "Epoch 00025: saving model to model_init_2020-05-0415_24_46.301761/model-00025-0.56879-0.73913-0.54922-0.65833.h5\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 7.450580950807417e-12.\n",
      "Epoch 26/30\n",
      "23/23 [==============================] - 41s 2s/step - loss: 0.3538 - categorical_accuracy: 0.8889 - val_loss: 0.7656 - val_categorical_accuracy: 0.6750\n",
      "\n",
      "Epoch 00026: saving model to model_init_2020-05-0415_24_46.301761/model-00026-0.35376-0.88889-0.76558-0.67500.h5\n",
      "Epoch 27/30\n",
      "23/23 [==============================] - 40s 2s/step - loss: 0.4670 - categorical_accuracy: 0.8068 - val_loss: 0.2848 - val_categorical_accuracy: 0.7917\n",
      "\n",
      "Epoch 00027: saving model to model_init_2020-05-0415_24_46.301761/model-00027-0.46702-0.80676-0.28483-0.79167.h5\n",
      "Epoch 28/30\n",
      "23/23 [==============================] - 40s 2s/step - loss: 0.4790 - categorical_accuracy: 0.8019 - val_loss: 1.0371 - val_categorical_accuracy: 0.7833\n",
      "\n",
      "Epoch 00028: saving model to model_init_2020-05-0415_24_46.301761/model-00028-0.47899-0.80193-1.03709-0.78333.h5\n",
      "Epoch 29/30\n",
      "23/23 [==============================] - 41s 2s/step - loss: 0.4256 - categorical_accuracy: 0.8357 - val_loss: 0.3248 - val_categorical_accuracy: 0.7750\n",
      "\n",
      "Epoch 00029: saving model to model_init_2020-05-0415_24_46.301761/model-00029-0.42561-0.83575-0.32480-0.77500.h5\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 3.725290475403709e-12.\n",
      "Epoch 30/30\n",
      "23/23 [==============================] - 41s 2s/step - loss: 0.4373 - categorical_accuracy: 0.8406 - val_loss: 0.5572 - val_categorical_accuracy: 0.8250\n",
      "\n",
      "Epoch 00030: saving model to model_init_2020-05-0415_24_46.301761/model-00030-0.43733-0.84058-0.55719-0.82500.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fb4700bcb90>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 30\n",
    "num_epochs = 30\n",
    "\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1\n",
    "    \n",
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "14/14 [==============================] - 25s 2s/step - loss: 0.3612 - categorical_accuracy: 0.9048 - val_loss: 0.5561 - val_categorical_accuracy: 0.7333\n",
      "\n",
      "Epoch 00001: saving model to model_init_2020-05-0415_24_46.301761/model-00001-0.36116-0.90476-0.55613-0.73333.h5\n",
      "Epoch 2/30\n",
      "14/14 [==============================] - 24s 2s/step - loss: 0.5587 - categorical_accuracy: 0.8095 - val_loss: 0.7101 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00002: saving model to model_init_2020-05-0415_24_46.301761/model-00002-0.55866-0.80952-0.71011-0.75000.h5\n",
      "Epoch 3/30\n",
      "14/14 [==============================] - 25s 2s/step - loss: 0.4992 - categorical_accuracy: 0.8175 - val_loss: 0.7491 - val_categorical_accuracy: 0.7667\n",
      "\n",
      "Epoch 00003: saving model to model_init_2020-05-0415_24_46.301761/model-00003-0.49916-0.81746-0.74914-0.76667.h5\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 4/30\n",
      "14/14 [==============================] - 25s 2s/step - loss: 0.5006 - categorical_accuracy: 0.8095 - val_loss: 0.7388 - val_categorical_accuracy: 0.6833\n",
      "\n",
      "Epoch 00004: saving model to model_init_2020-05-0415_24_46.301761/model-00004-0.50058-0.80952-0.73878-0.68333.h5\n",
      "Epoch 5/30\n",
      "14/14 [==============================] - 24s 2s/step - loss: 0.6113 - categorical_accuracy: 0.7540 - val_loss: 0.2986 - val_categorical_accuracy: 0.8500\n",
      "\n",
      "Epoch 00005: saving model to model_init_2020-05-0415_24_46.301761/model-00005-0.61135-0.75397-0.29858-0.85000.h5\n",
      "Epoch 6/30\n",
      "14/14 [==============================] - 24s 2s/step - loss: 0.6434 - categorical_accuracy: 0.7460 - val_loss: 0.5969 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00006: saving model to model_init_2020-05-0415_24_46.301761/model-00006-0.64343-0.74603-0.59688-0.75000.h5\n",
      "Epoch 7/30\n",
      "14/14 [==============================] - 24s 2s/step - loss: 0.3646 - categorical_accuracy: 0.8968 - val_loss: 0.2366 - val_categorical_accuracy: 0.8000\n",
      "\n",
      "Epoch 00007: saving model to model_init_2020-05-0415_24_46.301761/model-00007-0.36465-0.89683-0.23660-0.80000.h5\n",
      "Epoch 8/30\n",
      "14/14 [==============================] - 24s 2s/step - loss: 0.4621 - categorical_accuracy: 0.7937 - val_loss: 1.3888 - val_categorical_accuracy: 0.5667\n",
      "\n",
      "Epoch 00008: saving model to model_init_2020-05-0415_24_46.301761/model-00008-0.46207-0.79365-1.38885-0.56667.h5\n",
      "Epoch 9/30\n",
      "14/14 [==============================] - 24s 2s/step - loss: 0.4439 - categorical_accuracy: 0.8492 - val_loss: 0.2427 - val_categorical_accuracy: 0.9167\n",
      "\n",
      "Epoch 00009: saving model to model_init_2020-05-0415_24_46.301761/model-00009-0.44393-0.84921-0.24274-0.91667.h5\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "Epoch 10/30\n",
      "14/14 [==============================] - 24s 2s/step - loss: 0.5154 - categorical_accuracy: 0.7698 - val_loss: 0.5413 - val_categorical_accuracy: 0.7333\n",
      "\n",
      "Epoch 00010: saving model to model_init_2020-05-0415_24_46.301761/model-00010-0.51537-0.76984-0.54125-0.73333.h5\n",
      "Epoch 11/30\n",
      "14/14 [==============================] - 24s 2s/step - loss: 0.3428 - categorical_accuracy: 0.8810 - val_loss: 0.3765 - val_categorical_accuracy: 0.8000\n",
      "\n",
      "Epoch 00011: saving model to model_init_2020-05-0415_24_46.301761/model-00011-0.34277-0.88095-0.37649-0.80000.h5\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "Epoch 12/30\n",
      "14/14 [==============================] - 25s 2s/step - loss: 0.3864 - categorical_accuracy: 0.8651 - val_loss: 0.5143 - val_categorical_accuracy: 0.6500\n",
      "\n",
      "Epoch 00012: saving model to model_init_2020-05-0415_24_46.301761/model-00012-0.38638-0.86508-0.51429-0.65000.h5\n",
      "Epoch 13/30\n",
      "14/14 [==============================] - 24s 2s/step - loss: 0.4069 - categorical_accuracy: 0.8492 - val_loss: 0.5836 - val_categorical_accuracy: 0.8333\n",
      "\n",
      "Epoch 00013: saving model to model_init_2020-05-0415_24_46.301761/model-00013-0.40690-0.84921-0.58364-0.83333.h5\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "Epoch 14/30\n",
      "14/14 [==============================] - 24s 2s/step - loss: 0.4903 - categorical_accuracy: 0.8254 - val_loss: 0.5814 - val_categorical_accuracy: 0.8167\n",
      "\n",
      "Epoch 00014: saving model to model_init_2020-05-0415_24_46.301761/model-00014-0.49029-0.82540-0.58145-0.81667.h5\n",
      "Epoch 15/30\n",
      "14/14 [==============================] - 24s 2s/step - loss: 0.7270 - categorical_accuracy: 0.6587 - val_loss: 0.7185 - val_categorical_accuracy: 0.7167\n",
      "\n",
      "Epoch 00015: saving model to model_init_2020-05-0415_24_46.301761/model-00015-0.72700-0.65873-0.71847-0.71667.h5\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
      "Epoch 16/30\n",
      "14/14 [==============================] - 24s 2s/step - loss: 0.4484 - categorical_accuracy: 0.8016 - val_loss: 0.8673 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00016: saving model to model_init_2020-05-0415_24_46.301761/model-00016-0.44836-0.80159-0.86728-0.75000.h5\n",
      "Epoch 17/30\n",
      "14/14 [==============================] - 24s 2s/step - loss: 0.3851 - categorical_accuracy: 0.8571 - val_loss: 0.8634 - val_categorical_accuracy: 0.7833\n",
      "\n",
      "Epoch 00017: saving model to model_init_2020-05-0415_24_46.301761/model-00017-0.38513-0.85714-0.86343-0.78333.h5\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
      "Epoch 18/30\n",
      "14/14 [==============================] - 24s 2s/step - loss: 0.4259 - categorical_accuracy: 0.8651 - val_loss: 0.1611 - val_categorical_accuracy: 0.8500\n",
      "\n",
      "Epoch 00018: saving model to model_init_2020-05-0415_24_46.301761/model-00018-0.42592-0.86508-0.16107-0.85000.h5\n",
      "Epoch 19/30\n",
      "14/14 [==============================] - 24s 2s/step - loss: 0.3434 - categorical_accuracy: 0.8571 - val_loss: 0.8456 - val_categorical_accuracy: 0.6500\n",
      "\n",
      "Epoch 00019: saving model to model_init_2020-05-0415_24_46.301761/model-00019-0.34342-0.85714-0.84562-0.65000.h5\n",
      "Epoch 20/30\n",
      "14/14 [==============================] - 24s 2s/step - loss: 0.5613 - categorical_accuracy: 0.7698 - val_loss: 0.4178 - val_categorical_accuracy: 0.8000\n",
      "\n",
      "Epoch 00020: saving model to model_init_2020-05-0415_24_46.301761/model-00020-0.56132-0.76984-0.41779-0.80000.h5\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
      "Epoch 21/30\n",
      "14/14 [==============================] - 24s 2s/step - loss: 0.5768 - categorical_accuracy: 0.7619 - val_loss: 0.4313 - val_categorical_accuracy: 0.8000\n",
      "\n",
      "Epoch 00021: saving model to model_init_2020-05-0415_24_46.301761/model-00021-0.57678-0.76190-0.43134-0.80000.h5\n",
      "Epoch 22/30\n",
      "14/14 [==============================] - 23s 2s/step - loss: 0.4138 - categorical_accuracy: 0.8413 - val_loss: 0.3391 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00022: saving model to model_init_2020-05-0415_24_46.301761/model-00022-0.41380-0.84127-0.33907-0.75000.h5\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
      "Epoch 23/30\n",
      "14/14 [==============================] - 24s 2s/step - loss: 0.4525 - categorical_accuracy: 0.8651 - val_loss: 0.5853 - val_categorical_accuracy: 0.6667\n",
      "\n",
      "Epoch 00023: saving model to model_init_2020-05-0415_24_46.301761/model-00023-0.45253-0.86508-0.58530-0.66667.h5\n",
      "Epoch 24/30\n",
      "14/14 [==============================] - 24s 2s/step - loss: 0.6770 - categorical_accuracy: 0.7778 - val_loss: 0.4603 - val_categorical_accuracy: 0.7833\n",
      "\n",
      "Epoch 00024: saving model to model_init_2020-05-0415_24_46.301761/model-00024-0.67704-0.77778-0.46035-0.78333.h5\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
      "Epoch 25/30\n",
      "14/14 [==============================] - 24s 2s/step - loss: 0.4850 - categorical_accuracy: 0.7540 - val_loss: 0.4541 - val_categorical_accuracy: 0.8000\n",
      "\n",
      "Epoch 00025: saving model to model_init_2020-05-0415_24_46.301761/model-00025-0.48503-0.75397-0.45407-0.80000.h5\n",
      "Epoch 26/30\n",
      "14/14 [==============================] - 24s 2s/step - loss: 0.3513 - categorical_accuracy: 0.8413 - val_loss: 0.6429 - val_categorical_accuracy: 0.7667\n",
      "\n",
      "Epoch 00026: saving model to model_init_2020-05-0415_24_46.301761/model-00026-0.35131-0.84127-0.64287-0.76667.h5\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
      "Epoch 27/30\n",
      "14/14 [==============================] - 24s 2s/step - loss: 0.4294 - categorical_accuracy: 0.8175 - val_loss: 0.2620 - val_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00027: saving model to model_init_2020-05-0415_24_46.301761/model-00027-0.42942-0.81746-0.26203-0.70000.h5\n",
      "Epoch 28/30\n",
      "14/14 [==============================] - 24s 2s/step - loss: 0.4495 - categorical_accuracy: 0.8095 - val_loss: 0.3246 - val_categorical_accuracy: 0.8167\n",
      "\n",
      "Epoch 00028: saving model to model_init_2020-05-0415_24_46.301761/model-00028-0.44945-0.80952-0.32464-0.81667.h5\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 1.525878978725359e-08.\n",
      "Epoch 29/30\n",
      "14/14 [==============================] - 24s 2s/step - loss: 0.4141 - categorical_accuracy: 0.8254 - val_loss: 0.5469 - val_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00029: saving model to model_init_2020-05-0415_24_46.301761/model-00029-0.41411-0.82540-0.54692-0.70000.h5\n",
      "Epoch 30/30\n",
      "14/14 [==============================] - 24s 2s/step - loss: 0.3681 - categorical_accuracy: 0.8492 - val_loss: 0.6061 - val_categorical_accuracy: 0.8333\n",
      "\n",
      "Epoch 00030: saving model to model_init_2020-05-0415_24_46.301761/model-00030-0.36805-0.84921-0.60612-0.83333.h5\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 7.629394893626795e-09.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fb47005f550>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 50\n",
    "num_epochs = 30\n",
    "\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1\n",
    "    \n",
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model CNN-RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following is the model constructed for building a Conv2D + RNN model. \n",
    "from keras.layers import Conv2D, MaxPooling2D, LSTM\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(TimeDistributed(Conv2D(32, (7, 7), strides=(2, 2),\n",
    "            activation='relu', padding='same'), input_shape=input_shape))\n",
    "model.add(TimeDistributed(Conv2D(32, (3,3),\n",
    "            kernel_initializer=\"he_normal\", activation='relu')))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n",
    "\n",
    "model.add(TimeDistributed(Conv2D(64, (3,3),\n",
    "            padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(Conv2D(64, (3,3),\n",
    "            padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n",
    "\n",
    "model.add(TimeDistributed(Conv2D(128, (3,3),\n",
    "            padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(Conv2D(128, (3,3),\n",
    "            padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n",
    "\n",
    "model.add(TimeDistributed(Conv2D(256, (3,3),\n",
    "            padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(Conv2D(256, (3,3),\n",
    "            padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n",
    "        \n",
    "model.add(TimeDistributed(Conv2D(512, (3,3),\n",
    "            padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(Conv2D(512, (3,3),\n",
    "            padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n",
    "\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "model.add(LSTM(256, return_sequences=False, dropout=0.5))\n",
    "model.add(Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_17 (TimeDis (None, 30, 60, 60, 32)    4736      \n",
      "_________________________________________________________________\n",
      "time_distributed_18 (TimeDis (None, 30, 58, 58, 32)    9248      \n",
      "_________________________________________________________________\n",
      "time_distributed_19 (TimeDis (None, 30, 29, 29, 32)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_20 (TimeDis (None, 30, 29, 29, 64)    18496     \n",
      "_________________________________________________________________\n",
      "time_distributed_21 (TimeDis (None, 30, 29, 29, 64)    36928     \n",
      "_________________________________________________________________\n",
      "time_distributed_22 (TimeDis (None, 30, 14, 14, 64)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_23 (TimeDis (None, 30, 14, 14, 128)   73856     \n",
      "_________________________________________________________________\n",
      "time_distributed_24 (TimeDis (None, 30, 14, 14, 128)   147584    \n",
      "_________________________________________________________________\n",
      "time_distributed_25 (TimeDis (None, 30, 7, 7, 128)     0         \n",
      "_________________________________________________________________\n",
      "time_distributed_26 (TimeDis (None, 30, 7, 7, 256)     295168    \n",
      "_________________________________________________________________\n",
      "time_distributed_27 (TimeDis (None, 30, 7, 7, 256)     590080    \n",
      "_________________________________________________________________\n",
      "time_distributed_28 (TimeDis (None, 30, 3, 3, 256)     0         \n",
      "_________________________________________________________________\n",
      "time_distributed_29 (TimeDis (None, 30, 3, 3, 512)     1180160   \n",
      "_________________________________________________________________\n",
      "time_distributed_30 (TimeDis (None, 30, 3, 3, 512)     2359808   \n",
      "_________________________________________________________________\n",
      "time_distributed_31 (TimeDis (None, 30, 1, 1, 512)     0         \n",
      "_________________________________________________________________\n",
      "time_distributed_32 (TimeDis (None, 30, 512)           0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 30, 512)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 256)               787456    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 5)                 1285      \n",
      "=================================================================\n",
      "Total params: 5,504,805\n",
      "Trainable params: 5,504,805\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "optimiser = Adam() #optimizer to avoid the perfomance issue\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "34/34 [==============================] - 47s 1s/step - loss: 1.9144 - categorical_accuracy: 0.2353 - val_loss: 1.7488 - val_categorical_accuracy: 0.2133\n",
      "\n",
      "Epoch 00001: saving model to model_init_2020-05-0415_24_46.301761/model-00001-1.91440-0.23529-1.74876-0.21333.h5\n",
      "Epoch 2/20\n",
      "34/34 [==============================] - 40s 1s/step - loss: 1.8318 - categorical_accuracy: 0.1765 - val_loss: 1.8046 - val_categorical_accuracy: 0.2200\n",
      "\n",
      "Epoch 00002: saving model to model_init_2020-05-0415_24_46.301761/model-00002-1.83180-0.17647-1.80460-0.22000.h5\n",
      "Epoch 3/20\n",
      "34/34 [==============================] - 40s 1s/step - loss: 1.7560 - categorical_accuracy: 0.1961 - val_loss: 1.4921 - val_categorical_accuracy: 0.2600\n",
      "\n",
      "Epoch 00003: saving model to model_init_2020-05-0415_24_46.301761/model-00003-1.75597-0.19608-1.49209-0.26000.h5\n",
      "Epoch 4/20\n",
      "34/34 [==============================] - 42s 1s/step - loss: 1.7546 - categorical_accuracy: 0.1797 - val_loss: 1.7678 - val_categorical_accuracy: 0.2467\n",
      "\n",
      "Epoch 00004: saving model to model_init_2020-05-0415_24_46.301761/model-00004-1.75463-0.17974-1.76780-0.24667.h5\n",
      "Epoch 5/20\n",
      "34/34 [==============================] - 46s 1s/step - loss: 1.7283 - categorical_accuracy: 0.2157 - val_loss: 1.6096 - val_categorical_accuracy: 0.2267\n",
      "\n",
      "Epoch 00005: saving model to model_init_2020-05-0415_24_46.301761/model-00005-1.72826-0.21569-1.60959-0.22667.h5\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 6/20\n",
      "34/34 [==============================] - 43s 1s/step - loss: 1.7315 - categorical_accuracy: 0.1928 - val_loss: 1.6418 - val_categorical_accuracy: 0.1400\n",
      "\n",
      "Epoch 00006: saving model to model_init_2020-05-0415_24_46.301761/model-00006-1.73154-0.19281-1.64185-0.14000.h5\n",
      "Epoch 7/20\n",
      "34/34 [==============================] - 44s 1s/step - loss: 1.7207 - categorical_accuracy: 0.2320 - val_loss: 1.7594 - val_categorical_accuracy: 0.2133\n",
      "\n",
      "Epoch 00007: saving model to model_init_2020-05-0415_24_46.301761/model-00007-1.72067-0.23203-1.75935-0.21333.h5\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 8/20\n",
      "34/34 [==============================] - 44s 1s/step - loss: 1.6985 - categorical_accuracy: 0.2451 - val_loss: 1.8181 - val_categorical_accuracy: 0.1467\n",
      "\n",
      "Epoch 00008: saving model to model_init_2020-05-0415_24_46.301761/model-00008-1.69847-0.24510-1.81813-0.14667.h5\n",
      "Epoch 9/20\n",
      "34/34 [==============================] - 43s 1s/step - loss: 1.6803 - categorical_accuracy: 0.2026 - val_loss: 1.5981 - val_categorical_accuracy: 0.2067\n",
      "\n",
      "Epoch 00009: saving model to model_init_2020-05-0415_24_46.301761/model-00009-1.68030-0.20261-1.59813-0.20667.h5\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 10/20\n",
      "34/34 [==============================] - 46s 1s/step - loss: 1.6590 - categorical_accuracy: 0.2124 - val_loss: 1.6702 - val_categorical_accuracy: 0.2267\n",
      "\n",
      "Epoch 00010: saving model to model_init_2020-05-0415_24_46.301761/model-00010-1.65900-0.21242-1.67015-0.22667.h5\n",
      "Epoch 11/20\n",
      "34/34 [==============================] - 44s 1s/step - loss: 1.7012 - categorical_accuracy: 0.1797 - val_loss: 1.5315 - val_categorical_accuracy: 0.1800\n",
      "\n",
      "Epoch 00011: saving model to model_init_2020-05-0415_24_46.301761/model-00011-1.70118-0.17974-1.53155-0.18000.h5\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 12/20\n",
      "34/34 [==============================] - 43s 1s/step - loss: 1.7340 - categorical_accuracy: 0.2255 - val_loss: 1.6794 - val_categorical_accuracy: 0.1600\n",
      "\n",
      "Epoch 00012: saving model to model_init_2020-05-0415_24_46.301761/model-00012-1.73404-0.22549-1.67941-0.16000.h5\n",
      "Epoch 13/20\n",
      "34/34 [==============================] - 40s 1s/step - loss: 1.6511 - categorical_accuracy: 0.2549 - val_loss: 1.6758 - val_categorical_accuracy: 0.2133\n",
      "\n",
      "Epoch 00013: saving model to model_init_2020-05-0415_24_46.301761/model-00013-1.65108-0.25490-1.67576-0.21333.h5\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 14/20\n",
      "34/34 [==============================] - 42s 1s/step - loss: 1.7091 - categorical_accuracy: 0.2288 - val_loss: 1.8498 - val_categorical_accuracy: 0.2000\n",
      "\n",
      "Epoch 00014: saving model to model_init_2020-05-0415_24_46.301761/model-00014-1.70908-0.22876-1.84977-0.20000.h5\n",
      "Epoch 15/20\n",
      "34/34 [==============================] - 43s 1s/step - loss: 1.6397 - categorical_accuracy: 0.2386 - val_loss: 1.7678 - val_categorical_accuracy: 0.2400\n",
      "\n",
      "Epoch 00015: saving model to model_init_2020-05-0415_24_46.301761/model-00015-1.63974-0.23856-1.76784-0.24000.h5\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 16/20\n",
      "34/34 [==============================] - 45s 1s/step - loss: 1.7291 - categorical_accuracy: 0.2288 - val_loss: 1.6002 - val_categorical_accuracy: 0.2000\n",
      "\n",
      "Epoch 00016: saving model to model_init_2020-05-0415_24_46.301761/model-00016-1.72909-0.22876-1.60016-0.20000.h5\n",
      "Epoch 17/20\n",
      "34/34 [==============================] - 41s 1s/step - loss: 1.6598 - categorical_accuracy: 0.2647 - val_loss: 1.6862 - val_categorical_accuracy: 0.2333\n",
      "\n",
      "Epoch 00017: saving model to model_init_2020-05-0415_24_46.301761/model-00017-1.65981-0.26471-1.68623-0.23333.h5\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "Epoch 18/20\n",
      "34/34 [==============================] - 43s 1s/step - loss: 1.7853 - categorical_accuracy: 0.1895 - val_loss: 1.6704 - val_categorical_accuracy: 0.2133\n",
      "\n",
      "Epoch 00018: saving model to model_init_2020-05-0415_24_46.301761/model-00018-1.78530-0.18954-1.67039-0.21333.h5\n",
      "Epoch 19/20\n",
      "34/34 [==============================] - 44s 1s/step - loss: 1.7787 - categorical_accuracy: 0.1634 - val_loss: 1.6702 - val_categorical_accuracy: 0.1933\n",
      "\n",
      "Epoch 00019: saving model to model_init_2020-05-0415_24_46.301761/model-00019-1.77874-0.16340-1.67015-0.19333.h5\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "Epoch 20/20\n",
      "34/34 [==============================] - 41s 1s/step - loss: 1.7454 - categorical_accuracy: 0.1928 - val_loss: 1.6784 - val_categorical_accuracy: 0.2000\n",
      "\n",
      "Epoch 00020: saving model to model_init_2020-05-0415_24_46.301761/model-00020-1.74535-0.19281-1.67838-0.20000.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fb3305620d0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 20\n",
    "num_epochs = 20\n",
    "\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1\n",
    "    \n",
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Result for CNN-RNN is 20% accuracy which is not good for  this data. \n",
    "Conv3D is giving best result with accuracy of 82.5%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
